\documentclass[10pt,a4paper]{article}

% Standard required packages
\usepackage[utf8]{inputenc}
\usepackage{amsmath,bm,mathtools,amssymb} % math packages
\usepackage{listings}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{textgreek}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}


\usepackage{xcolor}

\definecolor{violet}{RGB}{148,0,211}
\definecolor{green}{RGB}{0,128,0}
\definecolor{red}{RGB}{255,0,0}
\lstdefinestyle{mystyle}{
    language=Python,
    basicstyle=\small,
    % basicstyle=\fontsize{7}{8}\selectfont,
    keywordstyle=\color{violet},
    commentstyle=\color{green},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny,
    numbersep=5pt,
    showstringspaces=false,
    breaklines=true,
    frame=leftline,
}
\lstset{style=mystyle}

\begin{document}

\begin{titlepage}
    \begin{center}
        \vspace*{1cm}
        \Huge\textbf{Assignment 2}\\
        \vspace{1.5cm}
        \Large Author:
        \textbf{Leonardo Colosi 1799057}\\
        \vspace{0.5cm}
        \Large Contributors: \textbf{Bruno Francesco Nocera 1863075, Silverio Manganaro 1817504, Simone Tozzi, 1615930, Paolo Renzi 1887793, Jacopo Tedeschi 1882789, Amine Ahardane 2050689.}
        \vfill
        \includegraphics[width=0.7\textwidth]{images/sapienza_logo.png}
        \vfill
        \vspace{0.8cm}
        \Large \textit{MARR, RL}\\
        \today
    \end{center}
\end{titlepage}
\newpage

\tableofcontents
\newpage

\section{Theory}
\subsection{Exercise 1}
Given the following table:
\vspace{5pt}
\begin{equation*} 
    Q(s,a) \:=\:
    \begin{pmatrix}
        Q(1,1) & Q(1,2)\\
        Q(2,1) & Q(2,2)
    \end{pmatrix}
    \:=\: 
    \begin{pmatrix}
        1 & 2\\
        3 & 4
    \end{pmatrix}
\end{equation*}
\vspace{5pt}

\noindent Assuming $\alpha \:=\: 0.1$ and $\gamma \:=\: 0.5$, after the experience: $(s, a, r, s^{\prime}) = (1,2,3,2)$ we can compute the Q-table update for:
\begin{enumerate}
    \item Q-Learning 
    \item SARSA in the case $a' \:=\: \pi_{\epsilon}(s^{\prime}) \:=\: 2$
\end{enumerate} 
1. In the case of Q-Learning we can proceed using the following update rule:
\begin{equation*}
    Q(s,a) = Q(s,a) \:+\: \alpha [ \:r \:+\: \gamma(max_{a}Q(s^{\prime},a')) \:-\: Q(s,a)]
\end{equation*}
So we would have:
\begin{align*}
    && Q(s,a) = Q(1,2) \:+\: 0.1 [ \:3 \:+\: 0.5(max_{a}Q(2,a')) \:-\: Q(1,2)] \\
    && \\
    &\rightarrow& Q(1,2) \:+\: 0.1 [ \:3 \:+\: 0.5(max(Q(2,1), \:Q(2,2))) \:-\: Q(1,2)] \\
    && \\
    &\rightarrow&2 \:+\: 0.1 [ \:3 \:+\: 0.5\cdot4 \:-\: 2] \\
    && \\
    &\rightarrow&2 \:+\: 0.3 = 2.3 \\
\end{align*}
\vspace{5pt}

\noindent 2. For SARSA we use as update rule:
\begin{equation*}
    Q(s,a) = Q(s,a) \:+\: \alpha [ \:r \:+\: \gamma(Q(s^{\prime},a')) \:-\: Q(s,a)]
\end{equation*}
in this case $a'=2$, so we would have:
\begin{align*}
    && Q(s,a) = Q(1,2) \:+\: 0.1 [ \:3 \:+\: 0.5(Q(2,a'))\:-\: Q(1,2)] \\
    && \\
    &\rightarrow& Q(1,2) \:+\: 0.1 [ \:3 \:+\: 0.5(Q(2,2))\:-\: Q(1,2)] \\
    && \\
    &\rightarrow&2 \:+\: 0.1 [ \:3 \:+\: 0.5\cdot4 \:-\: 2) \\
    && \\
    &\rightarrow&2 \:+\: 0.3 = 2.3 \\
\end{align*}
\newpage


\subsection{Exercise 2}
The goal of this exercise is to prove that the n-step error can also be written as a sum of TD errors if the value estimates do not
change from step to step. In other words we have to show that the following equation is true:
\begin{equation}
    G_{t:t+n} \:-\: V_{t+n-1}(S_t) \:=\: \sum_{k=t}^{t+n-1} \gamma^{k-t} \delta_{k}
\end{equation}
\vspace{5pt}

\noindent In order to proceed with the demonstration we can expand the left side of the equation by taking in account that: 
\begin{equation}
    G_{t:t+n}  \:=\: R_{t+1} \:+\: \gamma R_{t+2} \:+\: ... \:+\: \gamma^{n-1}R_{t+n} \:+\: \gamma^{n}V_{t+n-1}(S_{t+n}) 
\end{equation}
\vspace{5pt}
\noindent After making the expansion we obtain:
% \setcounter{equation}{0}
\begin{flalign*}
    &G_{t:t+n} \:-\: V_{t+n-1}(S_t) \:=\:  R_{t+1} \:+\: \gamma R_{t+2} \:+\: ... \:+\: \gamma^{n-1}R_{t+n} \:+\: \gamma^{n}V_{t+n-1}(S_{t+n}) \:-\: V_{t+n-1}(S_t)&\\
    &&\\
    &\:=\:  R_{t+1} \:+\: \gamma R_{t+2} \:+\: ... \:+\: \gamma^{n-1}R_{t+n} \:+\: \gamma^{n}V_{t+n-1}(S_{t+n}) \:-\: V_{t+n-1}(S_t)& 1.&\\
    &&\\
    &\:=\:  R_{t+1} \:+\: \gamma R_{t+2} \:+\: ... \:+\: \gamma^{n-1}R_{t+n} \:+\: \gamma^{n}V(S_{t+n}) \:-\: V(S_t) & 2.&\\
    &&\\
    &\:=\:  \delta_t \:-\: \gamma V(S_{t+1}) \:+\: V(S_t) \:+\: \gamma R_{t+2} \:+\: ... \:+\: \gamma^{n-1}R_{t+n} \:+\: \gamma^{n}V(S_{t+n}) \:-\: V(S_t) & 3.& \\
    &&\\
    &\:=\: [\delta_t \:-\: \gamma V(S_{t+1}) \:+\: V(S_t)] \:+\: \gamma[\delta_{t+1} \:-\: \gamma V(S_{t+2}) \:+\: V(S_{t+1})]  \:+\: ...&\\
    &... \:+\: \gamma^{n-1} [\delta_{t+n-1} \:-\: \gamma V(S_{t+n}) \:+\: V(S_{t+n-1})] \:+\: \gamma^{n}V(S_{t+n}) \:-\: V(S_t)& 4.& \\
    &&\\
    &\sum_{k=t}^{t+n-1} [\gamma^{k-t}\delta_k \:-\: \gamma^{k-t+1} V(S_{k+1}) \:+\: \gamma^{k-t}V(S_k)] \:+\: \gamma^{n}V(S_{t+n}) \:-\: V(S_t) & 5.& \\
    &&\\
    &\sum_{k=t}^{t+n-1} [\gamma^{k-t}\delta_k] \:-\: \sum_{k=t}^{t+n-1} [\gamma^{k-t+1} V(S_{k+1})] \:+\: \gamma^{n}V(S_{t+n}) \:+\: \sum_{k=t}^{t+n-1} [\gamma^{k-t}V(S_k)] \:-\: V(S_t) & 6.&
\end{flalign*}
\vspace{5pt}
\begin{itemize}
    \item[2.] Substituting $V_t$ with $V$, under the assumpion that it does not change.
    \item[3.] Recalling that $\delta_t  \:=\: R_{t+1} \:+\: \gamma V(S_{t+1}) \:-\: V(S_t)$, we can write the rewards in terms of $\delta$.
    \item[4.] Iterating the process of the previous steps for all the rewards $R_t$.
    \item[5.] Collecting the similar terms into a single summation.
    \item[6.] Splitting the summation in three parts, this will be useful to verify the presence of similar terms with opposite sign.
\end{itemize}
\newpage

\noindent We can rewrite the second summation at step 6. as: 
\begin{equation*}
    \:-\sum_{k=t}^{t+n-1} [\gamma^{k-t+1} V(S_{k+1})] \:+\: \gamma^{n}V(S_{t+n}) \:=\: \:-\sum_{k=t}^{t+n-2} [\gamma^{k-t+1} V(S_{k+1})] \:=\: \:-\sum_{k=t+1}^{t+n-1} [\gamma^{k-t}V(S_k)]
\end{equation*}
\vspace{5pt}
And the third summation at the same step as well:
\begin{equation*}
    \sum_{k=t}^{t+n-1} [\gamma^{k-t}V(S_k)] \:-\: V(S_t) \:=\: \sum_{k=t+1}^{t+n-1} [\gamma^{k-t}V(S_k)]
\end{equation*}
\vspace{5pt}
In this way our final equation becomes:
\begin{equation*}
    G_{t:t+n} \:-\: V_{t+n-1}(S_t) \:=\: \sum_{k=t}^{t+n-1} [\gamma^{k-t}\delta_k] \:-\sum_{k=t+1}^{t+n-1} [\gamma^{k-t}V(S_k)] \:+\: \sum_{k=t+1}^{t+n-1} [\gamma^{k-t}V(S_k)]
\end{equation*}
\vspace{5pt}
Which simplify in:
\begin{equation*}
    G_{t:t+n} \:-\: V_{t+n-1}(S_t) \:=\: \sum_{k=t}^{t+n-1} \gamma^{k-t}\delta_k
\end{equation*}

\newpage
\section{Code Implementation}

\subsection{SARSA-\textlambda{}}
In this exercise we have to resolve the \textit{"Taxi-v3"} environment from gymnasium, where the goal is to pick up the passenger from one of the colored squares and drop it off at the hotel. The problem is divided in two tasks:
\begin{itemize}
    \item[a.] Implementation of the $\epsilon$-greedy policy;
    \item[b.] Implementation of SARSA-$\lambda$, which means:
    \begin{enumerate}
        \item update of the Q table;
        \item update of the eligibility traces.
    \end{enumerate}
\end{itemize}
\vspace{5pt}

\noindent As for the first task here it is the code that choose an action with $\epsilon$-greedy strategy:
\begin{lstlisting}
def epsilon_greedy_action(env, Q, state, epsilon):
    if random.uniform(0, 1) < epsilon:
        action = env.action_space.sample()  
    else:
        action = argmax(Q[state])
    return action
\end{lstlisting}
\vspace{5pt}

\noindent Here the action is selected according to the result of a random sampling from an uniform probability distribution. If the sample value is lower than a given threshold $\epsilon$ the action will be choose randomly (\textit{exploration}) in the other case we will ches the best action accordingly to the q table (\textit{exploitation}).\\

\noindent Regarding the second point, the pseudo-code given at \textit{Slide} 71, \textit{Pack} 7 can be taken as a reference for the implementation of the update mechanism for both the Q table and the eligibility traces. Here it is the code for the update:
\begin{lstlisting}
delta = reward + (gamma*(1-done)*Q[next_s,next_a]) - Q[s,a]
E[s,a] += 1
Q = Q + alpha*delta*E
E = gamma*lambda_*E
\end{lstlisting}
In the first line is computed the TD error as: 
\begin{equation}
    \delta_t  \:=\: R_{t+1} \:+\: \gamma Q(s_{t+n}, a_{t+1}) \:-\: Q(s_t, a_t)
\end{equation}
then used it the update of the Q table. In the second line we add 1 to the entry of the eligibility trace relative to the current state-action pair, in order to take in account the frequency of the visits for a given state. The last two raws performs the actual update, in matrix from, of the two tables, accordingly to the update rules. In this way the learning part result much faster and the agent always reach its goal, usually taking the "safer" path.
\newpage
\subsection{RBF Q-Learning}
The aim of this second practical is to implement a \textit{RBF} encoder as well as the update rule for the Q-Learning algorithm. The goal of the agent (car) is to climb the hill and reach the green flag. The characteristics of the environment are specified on the gymnasium \href{https://gymnasium.farama.org/environments/classic_control/mountain_car/}{\underline{website}}. The observation is a collection of arrays where the elements correspond to position  along the x-axis and the velocity of the car.
 The implementation tasks are:
\begin{itemize}
    \item[a.] Code an RBG encoder to enhance relevant feature from the states;
    \item[b.] Code the backward update rule for Q-Learning with eligibility traces.
\end{itemize}

\noindent For the completion of the first it is useful to take in account the implementation of RBF on provided by sklearn. The two main function of the RBF encoder are the initialization and the actual encode, in  pseudo-code:
\begin{lstlisting}
def initialization(env):
    # Set up sklearn scaler function
    scaler = sklearn.preprocessing.StandardScaler()
    # Sampling a sequence of states to fit rbf
    for for num_of_sample:
        sampled_states = [env.observation_space.sample()]
    scaler.fit(sampled_states) # scale the sampled data
    # Set up of RBF encoder
    num_of_features <- set_number_of_features
    self.gamma <- initialization_of_gamma
    encoder =  RBFSampler(gamma, num_of_features)
    encoder.fit(self.scaler.transform(sampled_states))

def encode(state):
    # Use the rbf encoder to return the features
    scaled_state = scaler.transform(state)
    features = encoder.transform(scaled_state)
    # Flatten to restore the original state dimensionality
    return features.flatten() 
\end{lstlisting}

\noindent In the pseudo-code it can be notice how a sequence of random state is sampled from the observation space in order to "fit" the RBF encoder. This method is used to computes the parameters needed for the Radial Basis Function approximation. This involves determining the centers and widths of the RBFs based on the input data. In order to do that those data should be properly scaled, in this case this is achieved by the standard scaling function also provided by sklearn.

\newpage
\noindent For the second part of the task, the implementation of the update of the Q table (backward view) is done accordingly to the formulation provided in the \textit{Pack} 8 of the slides. 
\begin{lstlisting}
def update_transition(s, action, s_prime, reward, done):
    s_feats = feature_encoder.encode(s)
    s_prime_feats = feature_encoder.encode(s_prime)
    # Compute TD-Error
    td_error = reward + gamma*(1-done)*max(Q(s_prime_feats)) - Q(s_feats)[action]
    # Updtae all elegibility traces
    traces = self.gamma*self.lambda_*traces
    traces[action] += s_feats
    # Update the weights
    weights[action] += self.alpha*td_error*traces[action]
    # Reset elegibility traces when state is terminal
    if done: traces = zeros(self.shape)
\end{lstlisting}
In this case the agent learns to always take the best action, which usually means to allow going a bit backward first to gain the necessary momentum to climb.
\newpage

\begin{thebibliography}{9}
    \bibitem{texbook}
    \emph{Reinforcement Learning, second edition: An Introduction} by \textit{Richard S. Sutton}, \textit{Andrew G. Barto}, Chapters 6.1 and 7.1 

\end{thebibliography}


\end{document}
