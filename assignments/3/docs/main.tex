\documentclass[10pt,a4paper]{article}

% Standard required packages
\usepackage[utf8]{inputenc}
\usepackage{amsmath,bm,mathtools,amssymb} % math packages
\usepackage{listings}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{textgreek}
\usepackage{biblatex}
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\definecolor{violet}{RGB}{148,0,211}
\definecolor{green}{RGB}{0,128,0}
\definecolor{red}{RGB}{255,0,0}
\lstdefinestyle{mystyle}{
    language=Python,
    basicstyle=\small,
    % basicstyle=\fontsize{7}{8}\selectfont,
    keywordstyle=\color{violet},
    commentstyle=\color{green},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny,
    numbersep=5pt,
    showstringspaces=false,
    breaklines=true,
    frame=leftline,
}
\lstset{style=mystyle}

\addbibresource{bibliography.bib}

\begin{document}

\begin{titlepage}
    \begin{center}
        \vspace*{1cm}
        \Huge\textbf{Assignment 3}\\
        \vspace{1.5cm}
        \Large Author:
        \textbf{Leonardo Colosi 1799057}\\
        \vspace{0.5cm}
        \Large Main Contributors: \textbf{Bruno Francesco Nocera 1863075, Paolo Renzi 1887793} \\
        \vspace{0.3cm} 
        \Large Other Contributors: \textbf{Silverio Manganaro 1817504, Jacopo Tedeschi 1882789}
        \vfill
        \includegraphics[width=0.7\textwidth]{images/sapienza_logo.png}
        \vfill
        \vspace{0.8cm}
        \Large \textit{MARR, RL}\\
        \today
    \end{center}
\end{titlepage}
\newpage

\tableofcontents
\newpage

\section{Theory}
\subsection{Exercise 1}
In the context of this exercise, we need to consider an environment that allows two possible actions and features a two-dimensional state representation, denoted as $x(s)\in R^2$. This scenario involves the application of the 1-step Actor-Critic Algorithm with specific policy and action-state value function approximators:

\begin{gather*}
    \pi_{\theta}(a=1|s) \:=\: \sigma(\theta^{T} x(s)) \:=\: \frac{1}{1\:+\:e^{-(\theta^{T} x(s))}} \\
    Q_w (s,a=0) \:=\: w_{0}^{T}x(s) \\
    Q_w (s,a=1) \:=\: w_{1}^{T}x(s)
\end{gather*} 

\noindent Give the initial values for the weights and the values of the hyper-parameters:

\begin{gather*}
    w_{0} \:=\: (0.8, \:1)^{T},\: w_1 \:=\: (0.4, 0)^{T} \\
    \theta_{0} \: = \: (1, 0.5)^{T} \\
    \alpha_{w} \:=\: \alpha_{\theta} \:=\: \alpha \:=\: 0.1 \\
    \gamma \:=\: 0.9 
\end{gather*} 

\noindent As well as a defined transition:
\begin{center}
    $ x(s_0) = (1,0)^{T}, \: a_0=0, \: r_1=0, \:x(s_1) = (0,1)^{T}, \:a_{1} = 1 $
\end{center}

\vspace*{5pt}
\noindent The task is to compute new values of $w_0$, $w_1$ and $\theta$ after the given transition. To do this we have to follow the steps of the \textit{Q Actor-Critic} update method where:
\begin{enumerate}
    \item The weights of the "Critic" network \textbf{w} are updated to minimize the TD error;
    \item The weights of the "Actor" network $\theta$ are updated in the direction suggested by the Critic.
\end{enumerate} 

\noindent As first step we must compute the TD error, this can be done by the usual formula:
\begin{equation} \label{eq:TD}
    \delta = r \:+\: \gamma Q_{w}(s',\;a') - Q_{w}(s,\:a) 
\end{equation}
In this case ($s$, $a$) and ($s'$, $a'$) are given as $x(s_0)$, $a_{0}$ and $x(s_1)$, $a_{1}$. Now we have to evaluate the Q-function approximations for the given state-action pairs:
\begin{flalign*}
    Q_w (s,a = 0) \:=\: w_{0}^{T}x(s) \:=\: \begin{pmatrix}0.8 & 1\end{pmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix} \:=\: 0.8 &\\
    &\\
    Q_w (s,a = 1) \:=\: w_{1}^{T}x(s) \:=\: \begin{pmatrix}0.4 & 0\end{pmatrix} \begin{pmatrix} 0 \\ 1 \end{pmatrix} \:=\:  0 &
\end{flalign*}
Now we have all the numerical value to substitute in \ref{eq:TD}, after doing this we obtain:
\begin{equation*} 
    \delta = 0 \:+\: 0.9 \cdot (0) \:-\: 0.8 \:=\: -0.8  
\end{equation*}

\noindent For the update of the weight \textbf{w} we should compute the gradient of Q w.r.t. \textbf{w} itself. We can write the general formula as:
\begin{equation} 
    \mathbf{w} \:=\: \mathbf{w} \:+\: \alpha \gamma \nabla_{w} Q_{w} (s,a) 
\end{equation}
It is possible to decuple the computation of the new weights in two different equations:
\begin{equation*}
    w_{0} \:=\: w_{0} \:+\: \alpha \gamma \nabla_{w} Q_{w_0} (x(s_0), a_0) \:=\: \begin{pmatrix} 0.8 \\ 1 \end{pmatrix} \:-\: 
    0.8 \cdot 0.1 \cdot \begin{pmatrix} 1 \\ 0\end{pmatrix} \:=\: \begin{pmatrix} 0.72 \\ 1\end{pmatrix}
\end{equation*}

\noindent Regarding the second equation, the gradient with respect to $w_1$ of $Q(x(s), a_1)$ would be used to update $w_{1}$ only if the action $a_1 = 0$ was taken at state $x(s_0)$. In the given problem statement, since the action $a_0 = 0$ was taken at state $x(s)$, we would not use the gradient of $Q(x(s), a_1)$  for the update in this step. Using instead the gradient of $Q(x(s), a_0)$ lead to no change for the vector $w_1$:
\begin{equation*}
    w_{1} \:=\: w_{1} \:+\: \alpha \gamma \nabla_{w} Q_{w_1} (x(s), a_0) \:=\: \begin{pmatrix} 0.4 \\ 0 \end{pmatrix} \:-\: 
    0.8 \cdot 0.1 \cdot \begin{pmatrix} 0 \\ 0\end{pmatrix} \:=\: \begin{pmatrix} 0.4 \\ 0\end{pmatrix}
\end{equation*}
In certain variations of the Actor-Critic algorithm, one might update both $w_0$ and $w_1$ using the respective gradients even if one of the associated actions was not taken, as part of an off-policy learning method. This choice would lead to the following update:
\begin{equation*}
    w_{1} \:=\: w_{1} \:+\: \alpha \gamma \nabla_{w} Q_{w_1} (x(s_1), a_1) \:=\: \begin{pmatrix} 0.4 \\ 0 \end{pmatrix} \:-\: 
    0.8 \cdot 0.1 \cdot \begin{pmatrix} 0 \\ 1\end{pmatrix} \:=\: \begin{pmatrix} 0.4 \\ -0.08\end{pmatrix}
\end{equation*}

\noindent Finally we can compute the new value of $\theta$ according to:
\begin{equation}
    \theta \:=\: \theta \:+\: \alpha \delta \nabla_{\theta} \:\log \pi_{\theta} (a | s)
\end{equation}
In particular we need the expression for $\pi_{\theta}(a\:=\:0|s)$ in order to calculate the gradient. To obtain this expression we could make the observation that, since the action space is binary and $\pi$ represent a probability distribution over actions, 
$\pi_{\theta}(a=0|s) \:=\: 1 \:-\: \pi_{\theta}(a=1|s)$. 

Established this we can proceed with the computation of the gradient:
\vspace*{5pt}
\begin{flalign*}
    \nabla_{\theta} \: \log ( 1 - \pi_{\theta}(a=1|x(s_0))) = &&\\
    && &&\\
    \nabla_{\theta} \: \log \left( 1 - \frac{1}{1 + e^y} \right) \:=\: \nabla_{\theta} \: \log\left(\frac{1+e^{y}-1}{1 + e^y}\right) \:=\: && \text{where}\; \; y = -(\theta^{T} x(s)) \text{...}&& \\
    && &&\\
    \nabla_{\theta} \: \log\left(\frac{e^{y}}{1 + e^y}\right) \:=\:  \nabla_{\theta} \: \log(e^y) \:-\: \nabla_{\theta} \: \log\left(\frac{e^{y}}{1 + e^y}\right) \:=\: && \text{...splitting the logarithm ...} && \\\\
    && &&\\
    \nabla_{\theta} y \:-\: \nabla_{\theta} \: \log(1+e^y) \:=\: && \text{...substituting y...} && \\
    && &&\\
    - x(s) \:+\: \frac{x(s) \cdot e^{-(\theta^{T} x(s))}}{1 + e^{-(\theta^{T} x(s))}} \:=\:  && \text{...and taking the gradient.} &&\\
    && &&\\
    \frac{-x(s) - x(s) \cdot e^{-(\theta^{T} x(s))} + x(s) \cdot e^{-(\theta^{T} x(s))} }{1 + e^{-(\theta^{T} x(s))}} \:=\: && \text{Evaluating in} \:\: x(s_0), \: \theta_0 \text{...} &&\\
    && &&\\
    \frac{-x(s)}{1 + e^{-(\theta^{T} x(s))}} \:=\: - \begin{pmatrix} 0.73 \\ 0 \end{pmatrix}
\end{flalign*}

\noindent To obtain the final evaluation we should take in account that:
\begin{itemize}
    \item $\theta^{T} x(s) \:=\: 1$;
    \item $1+e^{-1} \:=\: 1.36$;
    \item $\frac{1}{1+e^{-1}} \:=\: 0.73$.
\end{itemize}
The new value for $\theta$ is given by:
\begin{equation*}
    \theta \:=\: \theta_0 \:+\: \alpha \delta \nabla_{\theta} \:\log \pi_{\theta} (a=0 | x(s_0))
    \:=\: \begin{pmatrix} 1 \\ 0.5 \end{pmatrix} \:+\: 0.1 \cdot 0.8 \cdot \begin{pmatrix} 0.73 \\ 0 \end{pmatrix} \:=\: \begin{pmatrix} 1.05 \\ 0.5 \end{pmatrix} 
\end{equation*}
\newpage 

\section{World Models}
 
\subsection{Task Overview}
The task of this assignment was to solve the CarRacing-v2 gym environment using one of the proposed algorithms. A complete description of the environment is reported on \href{https://www.gymlibrary.dev/environments/box2d/car_racing/}{\underline{gym  car-racing website}}. The generated track is random every episode, the observation states consists of 96x96 pixels images. The reward is $-0.1$ every frame and $+1000/N$ for every track tile visited, where N is the total number of tiles visited in the track. This means that agent would receive higher reward for completing the track in a shorter amount of time, this also means that in general moving is better than do nothing (even if sometimes it means to move off-track). From this observation we can conclude that the reward is not a perfect metric to evaluate the performance of the car but it's still a good way to influence its behavior. 

\subsection{Solution Strategy}
The strategy chosen among the proposed solution is the one implemented on \textit{World Models} \cite{DBLP:journals/corr/abs-1803-10122}, I along with my colleagues, listed as main Contributors, have implemented a simplified version of the model proposed in the paper. The main idea expressed in the paper is to combine three different models (see fig.1) in order to obtain the best performance from the agent. 
\begin{itemize}
    \item A Variational Auto Encoder: used for feature extraction and state representation;
    \item A MDN-RNN (LSTM): used to keep a memory of the , we also want to compress
    what happens over time. For this purpose, the role of the
    M model is to predict the future. The M model serves as a
    predictive model of the future
\end{itemize}
\newpage

\subsection{Code Structure}
The assignment directory is organized in modular components according to the implementation logic of World Models. Here it is a summary of the general structure:
\begin{itemize}
    \item \textbf{checkpoints}: shared directory to store the state dictionary of the various components of the model;
    \item \textbf{dataset}: local directory to store a collection of observation used to train the VAE;
    \item \textbf{modules}: contains the implementation of the architecture modules;
    \item \textbf{train}: is a directory that contains the files necessary to run the training of the NN modules;
    \item \textbf{utils}: is a collection of function used to perform random rollout, to gather and manipulate environment data and to handle the controller parameters;
    \item \textit{main.py}: the main file for the execution of both training and evaluation of the entire model (unchanged);
    \item \textit{student.py}: this file contains the implementation of the act function for the agent (\textit{Policy}) as well as the main function to train the controller.
\end{itemize}
In the following sections is reported a detailed explanation of the most relevant components of the project.

\subsection{Data Collection}
The task of collecting random frames from the environment, in order to give the agent a meaningful, generalized representation of the world, is handled by \textit{collect\_data.py}. This script is responsible of performing random rollouts, moving the agent in various direction and collecting a the desired number of frames (as long as the termination state is not encountered before). The random rollouts could be performed both in continuous and discrete settings, what change between the two cases is how the random policy select the actions.
\begin{itemize}
    \item \textit{Discrete case}: sample one random int form \{1,2,3,4\} $\rightarrow$ \{do nothing, steer left, steer right, gas, brake\}, in this case the action \{0: do nothing\} is discarded because it would be of no use to collect meaningful observations. 
    \item \textit{Continuous case}: sample one action from the continuous action space (dim=3) of the environment. To action is then added a weighted random sample extracted from a normal distribution to increase the change of performing "strange action in order to gather more generalized information. 
\end{itemize}
All data are then saved inside the local directory dataset, this directory is not contained in the zip file because of its size. The collected data are the one used to train the Neural Networks components of the architecture.
\newpage
\subsection{Variational Auto Encoder}
The Variational Auto Encoder (VAE) implemented for this assignment is a "vanilla" auto-encoder consisting of 3 \textit{Convolutional} layers and one \textit{Adaptive Pooling} for the encoder, two \textit{Linear} layers for computing $\mu$ and $\sigma$ and a combination of one \textit{Linear} layer and 3 \textit{Transposed Convolutional} for the decoder.
\subsubsection{VAE Training Strategy}
The training phase of the auto-encoder was carried on using different evaluation functions over the reconstructed images as inputs and the original images as target. The main loss used was the Kullback-Leibler Divergence (KDL) combined alteratively  with and the Mean Square Error (MSE) and the Weighted Binary Cross Entropy (BCE). The former performed best in terms of numerical loss values but the latter perform better in terms of final images reconstruction.\\
Here are the expression of the functions:
\begin{gather}
    \text{KLD} = -\frac{1}{2} \sum_{j=1}^J (1 + \log((\sigma_j)^2) - (\mu_j)^2 - (\sigma_j)^2) \\
    \text{MSE} = \frac{1}{N} \sum_{i=1}^N (y_i - \hat{y}_i)^2 \\
    \text{BCE} = -\frac{1}{N} \sum_{i=1}^N [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)]
\end{gather}
This plots show  the evolution of the loss functions during training.
INSERT PLOT HERE

\subsubsection{VAE Results}
The following are the results images reconstructed by the VAE. It can be notice that when the track is straight the network is able to reconstruct it pretty well, capturing also the features of the red racing curb. On the other hand, when there is a hairpin turn the results are poor, leading to bad performance of the car in this environmental conditions.
INSERT IMAGES HERE

\newpage

% \subsection{RNN}
% \newpage

\subsection*{Controller}
\newpage

\subsection{Results}
\newpage

\newpage


\printbibliography
% \begin{thebibliography}{9}
%     \bibitem{texbook}
%     \emph{Reinforcement Learning, second edition: An Introduction} by \textit{Richard S. Sutton}, \textit{Andrew G. Barto}, Chapters 6.1 and 7.1 

% \end{thebibliography}


\end{document}
