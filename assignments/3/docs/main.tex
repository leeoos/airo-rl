\documentclass[10pt,a4paper]{article}

% Standard required packages
\usepackage[utf8]{inputenc}
\usepackage{amsmath,bm,mathtools,amssymb} % math packages
\usepackage{listings}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{textgreek}
\usepackage{biblatex}
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\definecolor{violet}{RGB}{148,0,211}
\definecolor{green}{RGB}{0,128,0}
\definecolor{red}{RGB}{255,0,0}
\lstdefinestyle{mystyle}{
    language=Python,
    basicstyle=\small,
    % basicstyle=\fontsize{7}{8}\selectfont,
    keywordstyle=\color{violet},
    commentstyle=\color{green},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny,
    numbersep=5pt,
    showstringspaces=false,
    breaklines=true,
    frame=leftline,
}
\lstset{style=mystyle}

\addbibresource{bibliography.bib}

\begin{document}

\begin{titlepage}
    \begin{center}
        \vspace*{1cm}
        \Huge\textbf{Assignment 3}\\
        \vspace{1.5cm}
        \Large Author:
        \textbf{Leonardo Colosi 1799057}\\
        \vspace{0.5cm}
        \Large Contributors: \textbf{Bruno Francesco Nocera 1863075, Silverio Manganaro 1817504, Simone Tozzi, 1615930, Paolo Renzi 1887793, Jacopo Tedeschi 1882789, Amine Ahardane 2050689.}
        \vfill
        \includegraphics[width=0.7\textwidth]{images/sapienza_logo.png}
        \vfill
        \vspace{0.8cm}
        \Large \textit{MARR, RL}\\
        \today
    \end{center}
\end{titlepage}
\newpage

\tableofcontents
\newpage

\section{Theory}
\subsection{Exercise 1}

\noindent Assuming $\alpha \:=\: 0.1$ and $\gamma \:=\: 0.5$, after the experience: $(s, a, r, s^{\prime}) = (1,2,3,2)$ we can compute the Q-table update for:
\begin{enumerate}
    \item Q-Learning 
    \item SARSA in the case $a' \:=\: \pi_{\epsilon}(s^{\prime}) \:=\: 2$
\end{enumerate} 
1. In the case of Q-Learning we can proceed using the following update rule:
\begin{equation*}
    Q(s,a) = Q(s,a) \:+\: \alpha [ \:r \:+\: \gamma(max_{a}Q(s^{\prime},a')) \:-\: Q(s,a)]
\end{equation*}
So we would have:
\begin{align*}
    && Q(s,a) = Q(1,2) \:+\: 0.1 [ \:3 \:+\: 0.5(max_{a}Q(2,a')) \:-\: Q(1,2)] \\
    && \\
    &\rightarrow& Q(1,2) \:+\: 0.1 [ \:3 \:+\: 0.5(max(Q(2,1), \:Q(2,2))) \:-\: Q(1,2)] \\
    && \\
    &\rightarrow&2 \:+\: 0.1 [ \:3 \:+\: 0.5\cdot4 \:-\: 2] \\
    && \\
    &\rightarrow&2 \:+\: 0.3 = 2.3 \\
\end{align*}
\vspace{5pt}

\noindent 2. For SARSA we use as update rule:
\begin{equation*}
    Q(s,a) = Q(s,a) \:+\: \alpha [ \:r \:+\: \gamma(Q(s^{\prime},a')) \:-\: Q(s,a)]
\end{equation*}
in this case $a'=2$, so we would have:
\begin{align*}
    && Q(s,a) = Q(1,2) \:+\: 0.1 [ \:3 \:+\: 0.5(Q(2,a'))\:-\: Q(1,2)] \\
    && \\
    &\rightarrow& Q(1,2) \:+\: 0.1 [ \:3 \:+\: 0.5(Q(2,2))\:-\: Q(1,2)] \\
    && \\
    &\rightarrow&2 \:+\: 0.1 [ \:3 \:+\: 0.5\cdot4 \:-\: 2) \\
    && \\
    &\rightarrow&2 \:+\: 0.3 = 2.3 \\
\end{align*}
\newpage


\section{Code Implementation}
text example \cite{DBLP:journals/corr/abs-1803-10122} \cite{ctallec-wm}
 
\subsection{Task Overview}
\newpage

\subsection{Solution Strategy}
\newpage

\subsection{Code Structure}
\newpage

\subsection{VAE Implementation}
\newpage

% \subsection{RNN}
% \newpage

\subsection*{Controller}
\newpage

\subsection{Results}
\newpage

\begin{lstlisting}
def initialization(env):
    # Set up sklearn scaler function
    scaler = sklearn.preprocessing.StandardScaler()
    # Sampling a sequence of states to fit rbf
    for for num_of_sample:
        sampled_states = [env.observation_space.sample()]
    scaler.fit(sampled_states) # scale the sampled data
    # Set up of RBF encoder
    num_of_features <- set_number_of_features
    self.gamma <- initialization_of_gamma
    encoder =  RBFSampler(gamma, num_of_features)
    encoder.fit(self.scaler.transform(sampled_states))

def encode(state):
    # Use the rbf encoder to return the features
    scaled_state = scaler.transform(state)
    features = encoder.transform(scaled_state)
    # Flatten to restore the original state dimensionality
    return features.flatten() 
\end{lstlisting}
\newpage


\printbibliography
% \begin{thebibliography}{9}
%     \bibitem{texbook}
%     \emph{Reinforcement Learning, second edition: An Introduction} by \textit{Richard S. Sutton}, \textit{Andrew G. Barto}, Chapters 6.1 and 7.1 

% \end{thebibliography}


\end{document}
